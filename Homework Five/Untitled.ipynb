{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import glob #allows us to access every image in folder\n",
    "import re #allows us to extract emotion from file path\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image #allows us to resize images/get their initial pixel value\n",
    "from sklearn import preprocessing #allows us to normalize our data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a.i\n",
    "\n",
    "### Resizing images & Converting to np array\n",
    "\n",
    "Start by seeing what the initial pixel size is, because we want to get it down to a 30x30 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_pixels(filepath):\n",
    "    width, height = Image.open(filepath).size\n",
    "    return width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width:  320\n",
      "Height:  243\n"
     ]
    }
   ],
   "source": [
    "width_1_happy, height_1_happy = get_num_pixels(\"Images/subject01.happy.gif\")\n",
    "\n",
    "print(\"Width: \", width_1_happy)\n",
    "print(\"Height: \", height_1_happy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know all images have a size of 320x243 (even when their file sizes are different)... this will NOT resize in a way that keeps the aspect ratio in tact. Each image is surrounded by large portions of whitespace on the left or right, so we will chop that down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the size of the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're unable to maintain ratios and shrink down to a 30x30 size, we're going to chop off some of the extra white space on the images widths. We're going to get each image down to 243x243 so when we shrink to 30x30 nothing gets stretched out of proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_subjects = []\n",
    "\n",
    "for subject_num in range(1,16):\n",
    "    if subject_num < 10:\n",
    "        list_subjects.append(\"subject0\" + str(subject_num))\n",
    "    else:\n",
    "        list_subjects.append(\"subject\" + str(subject_num))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in a list of people whose image needs to be cropped in a similar fashion, and the dimensions of their cropping.\n",
    "\n",
    "It then crops it into a square so it can be resized into a 30x30 shape without any warping\n",
    "\n",
    "The image is then converted into a np array of its pixel values & added to our training data set. The label for each image is also added to an array (this is in the same order as our training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_resize(list_subjects, left, right, top, bottom):\n",
    "    subject_labels = []\n",
    "    training_data = []\n",
    "    \n",
    "    for subject in list_subjects:\n",
    "        for image_path in glob.glob(\"Images/\" + subject + \"*\"):\n",
    "            #for each image, extract the label\n",
    "            search_string = subject + \".(.+?).gif\"\n",
    "            m = re.search(search_string, image_path)\n",
    "            if m:\n",
    "                emotion = m.group(1)\n",
    "                subject_labels.append(subject + \": \" + emotion)\n",
    "            \n",
    "            #for each image, trim off the specified edges & save to new folder\n",
    "            crop_image_path = \"Images_Cropped/\" + subject + \".\" + emotion + \".gif\"\n",
    "            orig_image = Image.open(image_path)\n",
    "            orig_image.crop((left, top, right, bottom)).save(crop_image_path)\n",
    "            \n",
    "            #for each cropped image, resize to 30x30\n",
    "            resize_image_path = \"Images_Resized/\" + subject + \".\" + emotion + \".gif\"\n",
    "            basewidth = 30\n",
    "            crop_image = Image.open(crop_image_path)\n",
    "            wpercent = (basewidth / float(crop_image.size[0]))\n",
    "            hsize = int((float(crop_image.size[1]) * float(wpercent)))\n",
    "            resize_image = crop_image.resize((basewidth, hsize), PIL.Image.ANTIALIAS)\n",
    "            resize_image.save(resize_image_path)\n",
    "            \n",
    "            #for each resized image, convert to 1x90 np array\n",
    "            image_frame = Image.open(resize_image_path)\n",
    "            np_frame = np.array(image_frame.getdata()) #gets pixel values\n",
    "            image_list = list(np_frame)\n",
    "            training_data.append(image_list)\n",
    "\n",
    "    return subject_labels, training_data \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This organizes the subjects based on their cropping dimension needs. It's kind of a hack job, but it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_list = list_subjects[0:5]\n",
    "\n",
    "second_list = list_subjects[5:7]\n",
    "second_list.append(list_subjects[13])\n",
    "\n",
    "third_list = list_subjects[7:10]\n",
    "third_list.append(list_subjects[12])\n",
    "third_list.append(list_subjects[14])\n",
    "\n",
    "fourth_list = list_subjects[10:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates an overarching list for all training data and labels, and populates it with the ouput of our image resize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_training_data = []\n",
    "temp_labels = []\n",
    "\n",
    "first_labels, first_data = image_resize(first_list, 54, 297, 0, 243)\n",
    "temp_labels.append(first_labels)\n",
    "temp_training_data.append(first_data)\n",
    "\n",
    "second_labels, second_data = image_resize(second_list, 0, 243, 0, 243)\n",
    "temp_labels.append(second_labels)\n",
    "temp_training_data.append(second_data)\n",
    "\n",
    "third_labels, third_data = image_resize(third_list, 43, 286, 0, 243)\n",
    "temp_labels.append(third_labels)\n",
    "temp_training_data.append(third_data)\n",
    "\n",
    "fourth_labels, fourth_data = image_resize(fourth_list, 69, 312, 0, 243)\n",
    "temp_labels.append(fourth_labels)\n",
    "temp_training_data.append(fourth_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of nested sublists and convert training data to np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for sublist in temp_labels:\n",
    "    for item in sublist:\n",
    "        labels.append(item)\n",
    "\n",
    "training_data = []\n",
    "for sublist in temp_training_data:\n",
    "    for item in sublist:\n",
    "        training_data.append(item)\n",
    "\n",
    "training_data = np.array(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify we have 165 vectors of size 1x900 for our training data\n",
    "\n",
    "And an array of size 65 for our labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "(165, 900)\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(training_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows how the image is now represented by numbers. You cannot see it here because each row is broken up in the middle, but if you copy and past the output below into a text file and give each row its own line you will see the outline of a face :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject01: noglasses\n",
      "[213 213 213 213 213 213 213 213 213 213 213 213 213 213 213 213 213 213\n",
      " 213 213 213 213 213 213 213 213 213 213 213 213]\n",
      "[213 213 213 213 213 213 213 213 213 213 213 213 213 213 213 213 213 213\n",
      " 213 213 213 213 213 213 213 213 213 213 213 213]\n",
      "[213 213 213 213 213 213 213 213 213 213 213 213 213 213 213 206 198 120\n",
      "  98 113 143 213 213 213 213 213 213 213 213 213]\n",
      "[213 213 213 213 213 213 213 213 213 213 213 213 213 208 139  80  64  56\n",
      "  52  91 100 122 119 213 213 213 213 213 213 213]\n",
      "[213 213 213 213 213 213 213 213 213 213 213 178  67 101  62  69  37  39\n",
      "  81 102 105  89  78  91 213 213 213 213 213 213]\n",
      "[213 213 213 213 213 213 213 213 213 213 104 120  74  74  60  66  50  30\n",
      "  62  98 113  79  79 101  76 173 213 213 213 213]\n",
      "[213 213 213 213 213 213 213 213 213 119 126 139 104  79  88 153 111  27\n",
      "  31  83  89 110  78  71  76  70 191 213 213 213]\n",
      "[213 213 213 213 213 213 213 213 129 101  84  56  80 147 144 101 112  72\n",
      "  41  65  86  92  81  78  59  49  70 213 213 213]\n",
      "[213 213 213 213 213 213 213 213  84 105  69  57 118 156 137 127 114 126\n",
      "  78  69  84 101  72  46  51  59  57 138 213 213]\n",
      "[213 213 213 213 213 213 213 104  70  67  48  69 146 153 141 123 101  85\n",
      "  91 105  71  74  86  62  62  59  53  87 213 213]\n",
      "[213 213 213 213 213 213 213 144  36  35  54 107 152 167 146 115  93  93\n",
      "  96 105  80  71  69  72  36  38  50  65 213 213]\n",
      "[213 213 213 213 213 213 213 212  25  38  68  82 157 165 167 113 108  85\n",
      " 116 109 120  88  71  61  41  29  34  52 213 213]\n",
      "[213 213 213 213 213 213 213 213  50  40  39  85 165 179 168  84  95 103\n",
      " 133 126 102 111  89  73  18  22  43  57 213 213]\n",
      "[213 213 213 213 213 213 213 213  63  30  26 119 175 153 117  91 102 104\n",
      " 110 124 119 128 130 103  14  21  31  59 213 213]\n",
      "[213 213 213 213 213 213 213 213  49  32  16 135  98  63  29  20  57 108\n",
      " 118  55  21  25  57  92  49   7  20  61 213 213]\n",
      "[213 213 213 213 213 213 213 213  88  46  27 146  77  72  22  40  69 185\n",
      " 192  44  47   9  34  56  92   8  16  42 213 213]\n",
      "[213 213 213 213 213 213 213 213 124  58 119 176 170 157 170 155 181 188\n",
      " 170 164 133 113 103 128 129  12  34 102 213 213]\n",
      "[213 213 213 213 213 213 213 213 213 101 154 184 213 213 213 213 173 187\n",
      " 169 150 204 192 198 175 144  34  79 213 213 213]\n",
      "[213 213 213 213 213 213 213 213 213 151 158 145 166 211 213 213 159 193\n",
      " 208 144 190 213 183 140 123  96 191 213 213 213]\n",
      "[213 213 213 213 213 213 213 213 213 144 142 136 152 192 213 191  94  84\n",
      "  76  86 177 191 147 122 114  93 213 213 213 213]\n",
      "[213 213 213 213 213 213 213 213 213 213 137 135 146 170 199 162 124  88\n",
      "  72 108 151 167 133 116 109 126 213 213 185 213]\n",
      "[213 213 213 213 213 213 213 213 213 213  72 131 151 179 178 162 160 172\n",
      " 181 147 130 150 147 117 111  85 213 213 213 213]\n",
      "[213 213 213 213 213 213 213 213 213 148 179 136 134 166 146  67  38  40\n",
      "  37  44  66 125 140 114 105 213 213 213 213 213]\n",
      "[213 213 213 213 213 213 213 213 213 213 213 130 134 159 127 147 140  88\n",
      "  74 110 125 101 118 111 110 213 213 213 213 213]\n",
      "[213 213 213 213 213 213 213 213 213 213 213 109 128 128 119 107  94  80\n",
      "  84  88 101 103  98 103 136 199 213 213 213 213]\n",
      "[213 213 213 213 213 213 213 213 213 213 213 140 120 125 121 136 182 210\n",
      " 212 187 115 102 100  97 204 213 213 213 213 213]\n",
      "[213 213 213 213 213 213 213 213 213 213 213 190 118  90 111 154 155 156\n",
      " 139 151 130  96  76 149 213 213 213 211 213 213]\n",
      "[213 213 213 213 213 213 213 213 213 213 213 172 139  73  71  78  72  77\n",
      "  98  68  69  58  62 182 213 213 206 213 213 213]\n",
      "[213 213 213 213 213 213 213 213 213 213 161 173 146 122  71  67  61  59\n",
      "  56  57  58  54  85 146 213 213 213 213 213 213]\n",
      "[213 213 213 213 213 213 213 213 213 204 125 157 147 137 106  70  62  62\n",
      "  59  53  51  66 103 128 213 213 200 213 213 194]\n"
     ]
    }
   ],
   "source": [
    "print(str(labels[2])) #Prints letter being shown\n",
    "#Reshape image to be 14x9, then loop through each new row to print\n",
    "for row in training_data[2].reshape(30, 30):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts values to the range of 0:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_training_data = preprocessing.normalize(training_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
